{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<a id=\"section-one\"></a>\n# Training a fast.ai model\n","metadata":{"execution":{"iopub.status.busy":"2022-12-02T00:49:16.208242Z","iopub.execute_input":"2022-12-02T00:49:16.208915Z","iopub.status.idle":"2022-12-02T00:49:16.247884Z","shell.execute_reply.started":"2022-12-02T00:49:16.20876Z","shell.execute_reply":"2022-12-02T00:49:16.244677Z"}}},{"cell_type":"code","source":"!rm -r /kaggle/working/timm-with-dependencies\n!unzip -q ../input/timm-with-dependencies/timm_all -d timm-with-dependencies\n!pip install --no-index --find-links timm-with-dependencies timm\n!pip install /kaggle/input/dicomsdl-offline-installer/dicomsdl-0.109.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\n\nfrom fastai.vision.learner import *\nfrom fastai.data.all import *\nfrom fastai.vision.all import *\nfrom fastai.metrics import ActivationType\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom collections import defaultdict\nimport pandas as pd\nimport numpy as np\nfrom pdb import set_trace","metadata":{"execution":{"iopub.status.busy":"2023-02-10T09:33:26.608566Z","iopub.execute_input":"2023-02-10T09:33:26.609055Z","iopub.status.idle":"2023-02-10T09:34:37.311880Z","shell.execute_reply.started":"2023-02-10T09:33:26.609013Z","shell.execute_reply":"2023-02-10T09:34:37.310538Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Looking in links: timm-with-dependencies\nRequirement already satisfied: timm in /opt/conda/lib/python3.7/site-packages (0.6.12)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.7/site-packages (from timm) (0.10.1)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.7/site-packages (from timm) (0.12.0)\nRequirement already satisfied: torch>=1.7 in /opt/conda/lib/python3.7/site-packages (from timm) (1.11.0)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.7/site-packages (from timm) (6.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.7/site-packages (from torch>=1.7->timm) (4.1.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (3.7.1)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (4.13.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (4.64.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (2.28.1)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub->timm) (21.3)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (9.1.1)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from torchvision->timm) (1.21.6)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.9->huggingface-hub->timm) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->huggingface-hub->timm) (3.8.0)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (2022.9.24)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (1.26.12)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->huggingface-hub->timm) (3.3)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mProcessing /kaggle/input/dicomsdl-offline-installer/dicomsdl-0.109.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl\ndicomsdl is already installed with the same version as the provided wheel. Use --force-reinstall to force an installation of the wheel.\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"NUM_EPOCHS = 3\nNUM_SPLITS = 4\n\nRESIZE_TO = (512, 512)\n\nDATA_PATH = '/kaggle/input/rsna-breast-cancer-detection'\nTRAIN_IMAGE_DIR = '/kaggle/input/rsna-mammography-images-as-pngs/images_as_pngs_cv2_512'\nTEST_DICOM_DIR = '/kaggle/input/rsna-breast-cancer-detection/test_images'\n#MODEL_PATH = '/kaggle/input/rsna-trained-model-weights/tf_effv2_s_208_402/tf_effv2_s_208_402'\nMODEL_PATH = '/rsna-trained-model-weights-taaha/flw50-512-enet'\n\nlabel_smoothing_weights = torch.tensor([1,10]).float()\nif torch.cuda.is_available():\n    label_smoothing_weights = label_smoothing_weights.cuda()","metadata":{"execution":{"iopub.status.busy":"2023-02-10T09:34:45.228739Z","iopub.execute_input":"2023-02-10T09:34:45.229465Z","iopub.status.idle":"2023-02-10T09:34:47.436855Z","shell.execute_reply.started":"2023-02-10T09:34:45.229420Z","shell.execute_reply":"2023-02-10T09:34:47.435741Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"label_smoothing_weights","metadata":{"execution":{"iopub.status.busy":"2023-02-10T09:34:49.269535Z","iopub.execute_input":"2023-02-10T09:34:49.270039Z","iopub.status.idle":"2023-02-10T09:34:49.286864Z","shell.execute_reply.started":"2023-02-10T09:34:49.269998Z","shell.execute_reply":"2023-02-10T09:34:49.285615Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"tensor([ 1., 10.], device='cuda:0')"},"metadata":{}}]},{"cell_type":"markdown","source":"## Creating stratified splits for training","metadata":{}},{"cell_type":"code","source":"train_csv = pd.read_csv(f'{DATA_PATH}/train.csv')\npatient_id_any_cancer = train_csv.groupby('patient_id').cancer.max().reset_index()\nskf = StratifiedKFold(NUM_SPLITS, shuffle=True, random_state=42)\nsplits = list(skf.split(patient_id_any_cancer.patient_id, patient_id_any_cancer.cancer))","metadata":{"execution":{"iopub.status.busy":"2023-02-10T09:34:51.944231Z","iopub.execute_input":"2023-02-10T09:34:51.944737Z","iopub.status.idle":"2023-02-10T09:34:52.156522Z","shell.execute_reply.started":"2023-02-10T09:34:51.944693Z","shell.execute_reply":"2023-02-10T09:34:52.151861Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"train_csv.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-10T09:34:52.685880Z","iopub.execute_input":"2023-02-10T09:34:52.693955Z","iopub.status.idle":"2023-02-10T09:34:52.764581Z","shell.execute_reply.started":"2023-02-10T09:34:52.693909Z","shell.execute_reply":"2023-02-10T09:34:52.763553Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"   site_id  patient_id    image_id laterality view   age  cancer  biopsy  \\\n0        2       10006   462822612          L   CC  61.0       0       0   \n1        2       10006  1459541791          L  MLO  61.0       0       0   \n2        2       10006  1864590858          R  MLO  61.0       0       0   \n3        2       10006  1874946579          R   CC  61.0       0       0   \n4        2       10011   220375232          L   CC  55.0       0       0   \n\n   invasive  BIRADS  implant density  machine_id  difficult_negative_case  \n0         0     NaN        0     NaN          29                    False  \n1         0     NaN        0     NaN          29                    False  \n2         0     NaN        0     NaN          29                    False  \n3         0     NaN        0     NaN          29                    False  \n4         0     0.0        0     NaN          21                     True  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>site_id</th>\n      <th>patient_id</th>\n      <th>image_id</th>\n      <th>laterality</th>\n      <th>view</th>\n      <th>age</th>\n      <th>cancer</th>\n      <th>biopsy</th>\n      <th>invasive</th>\n      <th>BIRADS</th>\n      <th>implant</th>\n      <th>density</th>\n      <th>machine_id</th>\n      <th>difficult_negative_case</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2</td>\n      <td>10006</td>\n      <td>462822612</td>\n      <td>L</td>\n      <td>CC</td>\n      <td>61.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>29</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>10006</td>\n      <td>1459541791</td>\n      <td>L</td>\n      <td>MLO</td>\n      <td>61.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>29</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2</td>\n      <td>10006</td>\n      <td>1864590858</td>\n      <td>R</td>\n      <td>MLO</td>\n      <td>61.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>29</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2</td>\n      <td>10006</td>\n      <td>1874946579</td>\n      <td>R</td>\n      <td>CC</td>\n      <td>61.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>29</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2</td>\n      <td>10011</td>\n      <td>220375232</td>\n      <td>L</td>\n      <td>CC</td>\n      <td>55.0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0.0</td>\n      <td>0</td>\n      <td>NaN</td>\n      <td>21</td>\n      <td>True</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"patient_id_any_cancer.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-10T09:34:54.886096Z","iopub.execute_input":"2023-02-10T09:34:54.886538Z","iopub.status.idle":"2023-02-10T09:34:54.901088Z","shell.execute_reply.started":"2023-02-10T09:34:54.886500Z","shell.execute_reply":"2023-02-10T09:34:54.900093Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"   patient_id  cancer\n0           5       0\n1          25       0\n2          28       0\n3          30       0\n4          33       0","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>patient_id</th>\n      <th>cancer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>25</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>28</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>30</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>33</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"patient_id_any_cancer.shape","metadata":{"execution":{"iopub.status.busy":"2023-02-10T09:40:06.795583Z","iopub.execute_input":"2023-02-10T09:40:06.796116Z","iopub.status.idle":"2023-02-10T09:40:06.808539Z","shell.execute_reply.started":"2023-02-10T09:40:06.796072Z","shell.execute_reply":"2023-02-10T09:40:06.807541Z"},"trusted":true},"execution_count":19,"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"(11913, 2)"},"metadata":{}}]},{"cell_type":"code","source":"skf","metadata":{"execution":{"iopub.status.busy":"2023-02-10T09:34:56.368102Z","iopub.execute_input":"2023-02-10T09:34:56.368549Z","iopub.status.idle":"2023-02-10T09:34:56.381742Z","shell.execute_reply.started":"2023-02-10T09:34:56.368510Z","shell.execute_reply":"2023-02-10T09:34:56.380714Z"},"trusted":true},"execution_count":8,"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"StratifiedKFold(n_splits=4, random_state=42, shuffle=True)"},"metadata":{}}]},{"cell_type":"code","source":"splits","metadata":{"execution":{"iopub.status.busy":"2023-02-10T09:40:15.084881Z","iopub.execute_input":"2023-02-10T09:40:15.085349Z","iopub.status.idle":"2023-02-10T09:40:15.095956Z","shell.execute_reply.started":"2023-02-10T09:40:15.085311Z","shell.execute_reply":"2023-02-10T09:40:15.094683Z"},"trusted":true},"execution_count":20,"outputs":[{"execution_count":20,"output_type":"execute_result","data":{"text/plain":"[(array([    2,     3,     5, ..., 11909, 11910, 11912]),\n  array([    0,     1,     4, ..., 11898, 11904, 11911])),\n (array([    0,     1,     2, ..., 11908, 11911, 11912]),\n  array([    3,     9,    10, ..., 11903, 11909, 11910])),\n (array([    0,     1,     2, ..., 11909, 11910, 11911]),\n  array([    5,    16,    20, ..., 11906, 11907, 11912])),\n (array([    0,     1,     3, ..., 11910, 11911, 11912]),\n  array([    2,     6,     7, ..., 11901, 11905, 11908]))]"},"metadata":{}}]},{"cell_type":"code","source":"len(splits[0][1])","metadata":{"execution":{"iopub.status.busy":"2023-02-10T09:38:23.603794Z","iopub.execute_input":"2023-02-10T09:38:23.604244Z","iopub.status.idle":"2023-02-10T09:38:23.614804Z","shell.execute_reply.started":"2023-02-10T09:38:23.604207Z","shell.execute_reply":"2023-02-10T09:38:23.613800Z"},"trusted":true},"execution_count":15,"outputs":[{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"2979"},"metadata":{}}]},{"cell_type":"markdown","source":"## Defining some helper functions","metadata":{}},{"cell_type":"markdown","source":"I am defining some functionality here to make our life easier and to get us the last 10% of the way to a really good result.\n\nGenerally, none of this code is core to training or predicting, we could skip most of it and still be able to get a well trained model.\n\n\nBut here we want to push the boundaries of performance so let's get these things in 🙂","metadata":{}},{"cell_type":"code","source":"#https://www.kaggle.com/competitions/rsna-breast-cancer-detection/discussion/369267  \ndef pfbeta_torch(preds, labels, beta=1):\n    if preds.dim() != 2 or (preds.dim() == 2 and preds.shape[1] !=2): raise ValueError('Houston, we got a problem')\n    preds = preds[:, 1]\n    preds = preds.clip(0, 1)\n    y_true_count = labels.sum()\n    ctp = preds[labels==1].sum()\n    cfp = preds[labels==0].sum()\n    beta_squared = beta * beta\n    c_precision = ctp / (ctp + cfp)\n    c_recall = ctp / y_true_count\n    if (c_precision > 0 and c_recall > 0):\n        result = (1 + beta_squared) * (c_precision * c_recall) / (beta_squared * c_precision + c_recall)\n        return result\n    else:\n        return 0.0\n\n# https://www.kaggle.com/competitions/rsna-breast-cancer-detection/discussion/369886    \ndef pfbeta_torch_thresh(preds, labels):\n    optimized_preds = optimize_preds(preds, labels)\n    return pfbeta_torch(optimized_preds, labels)\n\ndef optimize_preds(preds, labels=None, thresh=None, return_thresh=False, print_results=False):\n    preds = preds.clone()\n    if labels is not None: without_thresh = pfbeta_torch(preds, labels)\n    \n    if not thresh and labels is not None:\n        threshs = np.linspace(0, 1, 101)\n        f1s = [pfbeta_torch((preds > thr).float(), labels) for thr in threshs]\n        idx = np.argmax(f1s)\n        thresh, best_pfbeta = threshs[idx], f1s[idx]\n\n    preds = (preds > thresh).float()\n\n    if print_results:\n        print(f'without optimization: {without_thresh}')\n        pfbeta = pfbeta_torch(preds, labels)\n        print(f'with optimization: {pfbeta}')\n        print(f'best_thresh = {thresh}')\n    if return_thresh:\n        return thresh\n    return preds\n\nfn2label = {fn: cancer_or_not for fn, cancer_or_not in zip(train_csv['image_id'].astype('str'), train_csv['cancer'])}\n\ndef splitting_func(paths):\n    train = []\n    valid = []\n    for idx, path in enumerate(paths):\n        if int(path.parent.name) in patient_id_any_cancer.iloc[splits[SPLIT][0]].patient_id.values:\n            train.append(idx)\n        else:\n            valid.append(idx)\n    return train, valid\n\ndef label_func(path):\n    return fn2label[path.stem]\n\ndef get_items(image_dir_path):\n    items = []\n    for p in get_image_files(image_dir_path):\n        items.append(p)\n        if p.stem in fn2label and int(p.parent.name) in patient_id_any_cancer.iloc[splits[SPLIT][0]].patient_id.values:\n            if label_func(p) == 1:\n                for _ in range(5):\n                    items.append(p)\n    return items","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-02-10T09:30:42.750342Z","iopub.status.idle":"2023-02-10T09:30:42.758015Z","shell.execute_reply.started":"2023-02-10T09:30:42.757750Z","shell.execute_reply":"2023-02-10T09:30:42.757775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Wrapping getting data and getting a model into functions -- this way our logic for training will be cleaner to read.","metadata":{}},{"cell_type":"code","source":"from timm.models.layers.adaptive_avgmax_pool import SelectAdaptivePool2d\nfrom torch.nn import Flatten\n\ndef get_dataloaders():\n    train_image_path = TRAIN_IMAGE_DIR\n\n    dblock = DataBlock(\n        blocks    = (ImageBlock, CategoryBlock),\n        get_items = get_items,\n        get_y = label_func,\n        splitter  = splitting_func,\n        #batch_tfms=[Flip()],\n    )\n    dsets = dblock.datasets(train_image_path)\n    return dblock.dataloaders(train_image_path, batch_size=32)\n\ndef get_learner(arch=resnet18):\n    learner = vision_learner(\n        get_dataloaders(),\n        arch,\n        custom_head=nn.Sequential(SelectAdaptivePool2d(pool_type='avg', flatten=Flatten()), nn.Linear(1280, 2)),\n        metrics=[\n            error_rate,\n            accuracy,\n            F1Score()\n#             error_rate,\n#             AccumMetric(pfbeta_torch, activation=ActivationType.Softmax, flatten=False),\n#             AccumMetric(pfbeta_torch_thresh, activation=ActivationType.Softmax, flatten=False)\n        ],\n        loss_func=FocalLossFlat(gamma=2),\n#        CrossEntropyLossFlat(weight=torch.tensor([1,50]).float()),\n        pretrained=True,\n        #normalize=False\n    ).to_fp16()\n    return learner","metadata":{"execution":{"iopub.status.busy":"2023-02-10T09:30:42.759531Z","iopub.status.idle":"2023-02-10T09:30:42.760318Z","shell.execute_reply.started":"2023-02-10T09:30:42.760049Z","shell.execute_reply":"2023-02-10T09:30:42.760074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Creating the learner and training","metadata":{}},{"cell_type":"code","source":"# This is a dependency that is needed for reading DICOM images\n\ntry:\n    import pylibjpeg\nexcept:\n    !rm -rf /root/.cache/torch/hub/checkpoints/\n    !mkdir -p /root/.cache/torch/hub/checkpoints/\n    !pip install /kaggle/input/rsna-2022-whl/{pydicom-2.3.0-py3-none-any.whl,pylibjpeg-1.4.0-py3-none-any.whl,python_gdcm-3.0.15-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl}\n    !pip install /kaggle/input/rsna-2022-whl/{torch-1.12.1-cp37-cp37m-manylinux1_x86_64.whl,torchvision-0.13.1-cp37-cp37m-manylinux1_x86_64.whl}\n\n# copying the pretrained weights\n\nif not os.path.exists('/root/.cache/torch/hub/checkpoints/'):\n        os.makedirs('/root/.cache/torch/hub/checkpoints/')\n!cp '/kaggle/input/pretrained-model-weights-for-fastai/resnet18-f37072fd.pth' '/root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth'\n!cp '/kaggle/input/pretrained-model-weights-for-fastai/tf_efficientnetv2_s-eb54923e.pth' '/root/.cache/torch/hub/checkpoints/tf_efficientnetv2_s-eb54923e.pth'","metadata":{"execution":{"iopub.status.busy":"2023-02-10T09:30:42.862526Z","iopub.status.idle":"2023-02-10T09:30:42.863147Z","shell.execute_reply.started":"2023-02-10T09:30:42.862855Z","shell.execute_reply":"2023-02-10T09:30:42.862881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\npreds, labels = [], []\n\nSPLIT = 0 # our learner needs this to construct its dataloaders...\nlearn = get_learner('tf_efficientnetv2_s')\n\n# instead of training, to conserve pipeline time, I am uploading models trained locally\n# uncomment the lines below for training\n  \nfor SPLIT in range(NUM_SPLITS):\n    learn = get_learner()\n    learn.fit_one_cycle(6, 0.001)\n#     learn.unfreeze()\n#     learn.fit_one_cycle(NUM_EPOCHS, 1e-4, pct_start=0.1)\n    learn.save(f'{MODEL_PATH}/{SPLIT}')\n        \n    output = learn.get_preds()\n    preds.append(output[0])\n    labels.append(output[1])","metadata":{"execution":{"iopub.status.busy":"2023-02-10T09:30:42.864105Z","iopub.status.idle":"2023-02-10T09:30:42.864569Z","shell.execute_reply.started":"2023-02-10T09:30:42.864331Z","shell.execute_reply":"2023-02-10T09:30:42.864354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# threshold = optimize_preds(torch.cat(preds), torch.cat(labels), return_thresh=True, print_results=True)\n#threshold = 0.402","metadata":{"execution":{"iopub.status.busy":"2023-02-10T09:30:42.866992Z","iopub.status.idle":"2023-02-10T09:30:42.867802Z","shell.execute_reply.started":"2023-02-10T09:30:42.867518Z","shell.execute_reply":"2023-02-10T09:30:42.867543Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Predicting on test<a id=\"section-two\">","metadata":{}},{"cell_type":"code","source":"# import pydicom\n# from pydicom.pixel_data_handlers.util import apply_voi_lut\nimport dicomsdl\n    \nfrom pathlib import Path\nimport multiprocessing as mp\nimport cv2\n\n!rm -rf test_resized_{RESIZE_TO[0]}\n\ndef dicom_file_to_ary(path):\n    dcm_file = dicomsdl.open(str(path))\n    data = dcm_file.pixelData()\n\n    data = (data - data.min()) / (data.max() - data.min())\n\n    if dcm_file.getPixelDataInfo()['PhotometricInterpretation'] == \"MONOCHROME1\":\n        data = 1 - data\n\n    data = cv2.resize(data, RESIZE_TO)\n    data = (data * 255).astype(np.uint8)\n    return data\n\ndirectories = list(Path(TEST_DICOM_DIR).iterdir())\n\ndef process_directory(directory_path):\n    parent_directory = str(directory_path).split('/')[-1]\n    !mkdir -p test_resized_{RESIZE_TO[0]}/{parent_directory}\n    for image_path in directory_path.iterdir():\n        processed_ary = dicom_file_to_ary(image_path)\n        cv2.imwrite(\n            f'test_resized_{RESIZE_TO[0]}/{parent_directory}/{image_path.stem}.png',\n            processed_ary\n        )\n\nwith mp.Pool(mp.cpu_count()) as p:\n    p.map(process_directory, directories)","metadata":{"_kg_hide-input":true,"execution":{"iopub.status.busy":"2023-02-10T09:30:42.869200Z","iopub.status.idle":"2023-02-10T09:30:42.869991Z","shell.execute_reply.started":"2023-02-10T09:30:42.869737Z","shell.execute_reply":"2023-02-10T09:30:42.869762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"%%time\n\npreds_all = []\n\ntest_dl = learn.dls.test_dl(get_image_files(f'test_resized_{RESIZE_TO[0]}'))\nfor SPLIT in range(NUM_SPLITS):\n    learn.load(f'{MODEL_PATH}/{SPLIT}')\n    preds, _ = learn.get_preds(dl=test_dl)\n    preds_all.append(preds)","metadata":{"execution":{"iopub.status.busy":"2023-02-10T09:30:42.877503Z","iopub.status.idle":"2023-02-10T09:30:42.878287Z","shell.execute_reply.started":"2023-02-10T09:30:42.878026Z","shell.execute_reply":"2023-02-10T09:30:42.878051Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"preds = torch.zeros_like(preds_all[0])\nfor pred in preds_all:\n    preds += pred\n\npreds /= NUM_SPLITS\n\n\npreds = optimize_preds(preds, thresh=threshold)\nimage_ids = [path.stem for path in test_dl.items]\n\nimage_id2pred = defaultdict(lambda: 0)\nfor image_id, pred in zip(image_ids, preds[:, 1]):\n    image_id2pred[int(image_id)] = pred.item()","metadata":{"execution":{"iopub.status.busy":"2023-02-10T09:30:42.879713Z","iopub.status.idle":"2023-02-10T09:30:42.880488Z","shell.execute_reply.started":"2023-02-10T09:30:42.880216Z","shell.execute_reply":"2023-02-10T09:30:42.880240Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<a id=\"section-three\"></a>\n# Making a submission","metadata":{}},{"cell_type":"code","source":"test_csv = pd.read_csv('/kaggle/input/rsna-breast-cancer-detection/test.csv')\n\nprediction_ids = []\npreds = []\n\nfor _, row in test_csv.iterrows():\n    prediction_ids.append(row.prediction_id)\n    preds.append(image_id2pred[row.image_id])\n\nsubmission = pd.DataFrame(data={'prediction_id': prediction_ids, 'cancer': preds}).groupby('prediction_id').max().reset_index()\nsubmission.head()","metadata":{"execution":{"iopub.status.busy":"2023-02-10T09:30:42.881938Z","iopub.status.idle":"2023-02-10T09:30:42.882751Z","shell.execute_reply.started":"2023-02-10T09:30:42.882474Z","shell.execute_reply":"2023-02-10T09:30:42.882499Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"submission.to_csv('submission.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2023-02-10T09:30:42.884130Z","iopub.status.idle":"2023-02-10T09:30:42.884933Z","shell.execute_reply.started":"2023-02-10T09:30:42.884681Z","shell.execute_reply":"2023-02-10T09:30:42.884706Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"And that's it! Thank you very much for reading! 🙂\n\n**If you enjoyed the notebook, please upvote! 🙏 Thank you, appreciate your support!**\n\nHappy Kaggling 🥳\n","metadata":{}}]}